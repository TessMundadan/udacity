{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "There are lot of people travelling to US every year for businesss, pleasure and studies.\n",
    "The project is to find relationship between US immigration, the weather in the city and the demography of the city.\n",
    "\n",
    "The project follows the following steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n",
    "* Step 6 Analytics performed on the fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "import datetime\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import datediff, to_date\n",
    "from pyspark.sql.functions import year, month\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "The project scope is to identify any correlation between number of people travelling to a city in US for business,pleasure,study and  weather in the city. We also try to identify if the number of visitors have any relation to the demography of immigrant population in that city.\n",
    "For this project, data being used are \n",
    "1. Immigration data\n",
    "2. Temperature Data \n",
    "3. US Cities: Demographics\n",
    "4. Airport Codes\n",
    "\n",
    "\n",
    "At the end the goal is to get the number of immigrant travelling to US per city per month, get the weather for the city for that month , also get the count of immigrant population in that city.We do so by joining the immigration dataset with airport dataset on the port of entry column,then join the temperature and demographics dataset using the city column.\n",
    "\n",
    "The tools used in the project are:\n",
    "* AWS S3\n",
    "* Spark\n",
    "* EMR\n",
    "#### Describe and Gather Data \n",
    "\n",
    "I94 Immigration Data: https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "This data comes from the US National Tourism and Trade Office.This dataset contains information about port of entry,type of visa, mode of transportation, age groups, states intended to visit and country of origin.\n",
    "\n",
    "World Temperature Data: https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "This dataset came from Kaggle. We use dataset has land temperatures by city for each month.\n",
    "\n",
    "U.S. City Demographic https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/\n",
    "This data comes from OpenSoft.This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. This data comes from the US Census Bureau's 2015 American Community Survey.\n",
    "\n",
    "Airport Code Table: https://datahub.io/core/airport-codes#data\n",
    "This is a simple table of airport codes and corresponding cities. The airport codes may refer to either IATA airport code, a three-letter code which is used in passenger reservation, ticketing and baggage-handling systems, or the ICAO airport code which is a four letter code used by ATC systems and for airports that do not have an IATA airport code.\n",
    "\n",
    "#### Assumption\n",
    "The dataset are collected at different year. Assumption is made that the data is collected at the same year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read immigration SAS data\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_pd_immi = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd_immi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read temperature source file\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_pd_temp = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8599207</th>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>11.464</td>\n",
       "      <td>0.236</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599208</th>\n",
       "      <td>2013-06-01</td>\n",
       "      <td>15.043</td>\n",
       "      <td>0.261</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599209</th>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>18.775</td>\n",
       "      <td>0.193</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599210</th>\n",
       "      <td>2013-08-01</td>\n",
       "      <td>18.025</td>\n",
       "      <td>0.298</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599211</th>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>5.26E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 dt  AverageTemperature  AverageTemperatureUncertainty  \\\n",
       "8599207  2013-05-01              11.464                          0.236   \n",
       "8599208  2013-06-01              15.043                          0.261   \n",
       "8599209  2013-07-01              18.775                          0.193   \n",
       "8599210  2013-08-01              18.025                          0.298   \n",
       "8599211  2013-09-01                 NaN                            NaN   \n",
       "\n",
       "           City      Country Latitude Longitude  \n",
       "8599207  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599208  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599209  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599210  Zwolle  Netherlands   52.24N     5.26E  \n",
       "8599211  Zwolle  Netherlands   52.24N     5.26E  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read demographics source file\n",
    "fname = 'us-cities-demographics.csv'\n",
    "df_pd_city = pd.read_csv(fname,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd_city.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read airport source file\n",
    "fname = 'airport-codes_csv.csv'\n",
    "df_pd_airport = pd.read_csv(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd_airport.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "To explore the data functions of pandas dataframe are used.\n",
    "1. df.count() -- To get count of non null values for each column\n",
    "2. df.isnull().sum()  -- To get a count of how many null values there are in each column.\n",
    "\n",
    "#### Cleaning Steps\n",
    "The following steps would be done after reading the files to a spark dataframe\n",
    "1. Select only the required columns from the dataset\n",
    "2. Remove Null columns\n",
    "3. Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cicid       3096313\n",
       "i94yr       3096313\n",
       "i94mon      3096313\n",
       "i94cit      3096313\n",
       "i94res      3096313\n",
       "i94port     3096313\n",
       "arrdate     3096313\n",
       "i94mode     3096074\n",
       "i94addr     2943941\n",
       "depdate     2953856\n",
       "i94bir      3095511\n",
       "i94visa     3096313\n",
       "count       3096313\n",
       "dtadfile    3096312\n",
       "visapost    1215063\n",
       "occup          8126\n",
       "entdepa     3096075\n",
       "entdepd     2957884\n",
       "entdepu         392\n",
       "matflag     2957884\n",
       "biryear     3095511\n",
       "dtaddto     3095836\n",
       "gender      2682044\n",
       "insnum       113708\n",
       "airline     3012686\n",
       "admnum      3096313\n",
       "fltno       3076764\n",
       "visatype    3096313\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing cleaning tasks here\n",
    "# Get count of non null values for each column\n",
    "df_pd_immi.count()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cicid             0\n",
       "i94yr             0\n",
       "i94mon            0\n",
       "i94cit            0\n",
       "i94res            0\n",
       "i94port           0\n",
       "arrdate           0\n",
       "i94mode         239\n",
       "i94addr      152372\n",
       "depdate      142457\n",
       "i94bir          802\n",
       "i94visa           0\n",
       "count             0\n",
       "dtadfile          1\n",
       "visapost    1881250\n",
       "occup       3088187\n",
       "entdepa         238\n",
       "entdepd      138429\n",
       "entdepu     3095921\n",
       "matflag      138429\n",
       "biryear         802\n",
       "dtaddto         477\n",
       "gender       414269\n",
       "insnum      2982605\n",
       "airline       83627\n",
       "admnum            0\n",
       "fltno         19549\n",
       "visatype          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get count of null values for each column\n",
    "df_pd_immi.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                               8599212\n",
       "AverageTemperature               8235082\n",
       "AverageTemperatureUncertainty    8235082\n",
       "City                             8599212\n",
       "Country                          8599212\n",
       "Latitude                         8599212\n",
       "Longitude                        8599212\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get count of non null values for each column \n",
    "df_pd_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                                    0\n",
       "AverageTemperature               364130\n",
       "AverageTemperatureUncertainty    364130\n",
       "City                                  0\n",
       "Country                               0\n",
       "Latitude                              0\n",
       "Longitude                             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get count of null values for columns\n",
    "df_pd_temp.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                      2891\n",
       "State                     2891\n",
       "Median Age                2891\n",
       "Male Population           2888\n",
       "Female Population         2888\n",
       "Total Population          2891\n",
       "Number of Veterans        2878\n",
       "Foreign-born              2878\n",
       "Average Household Size    2875\n",
       "State Code                2891\n",
       "Race                      2891\n",
       "Count                     2891\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get count of non null values for each column\n",
    "df_pd_city.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                       0\n",
       "State                      0\n",
       "Median Age                 0\n",
       "Male Population            3\n",
       "Female Population          3\n",
       "Total Population           0\n",
       "Number of Veterans        13\n",
       "Foreign-born              13\n",
       "Average Household Size    16\n",
       "State Code                 0\n",
       "Race                       0\n",
       "Count                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get count of null values for columns\n",
    "df_pd_city.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident           55075\n",
       "type            55075\n",
       "name            55075\n",
       "elevation_ft    48069\n",
       "continent       27356\n",
       "iso_country     54828\n",
       "iso_region      55075\n",
       "municipality    49399\n",
       "gps_code        41030\n",
       "iata_code        9189\n",
       "local_code      28686\n",
       "coordinates     55075\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get count of non null values for each column\n",
    "df_pd_airport.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident               0\n",
       "type                0\n",
       "name                0\n",
       "elevation_ft     7006\n",
       "continent       27719\n",
       "iso_country       247\n",
       "iso_region          0\n",
       "municipality     5676\n",
       "gps_code        14045\n",
       "iata_code       45886\n",
       "local_code      26389\n",
       "coordinates         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get count of null values for columns\n",
    "df_pd_airport.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Please refer to Conceptual_Data_model.pdf \n",
    "\n",
    "We use a star schema.\n",
    "We create the following dimension tables from the source:\n",
    "*  temperature_table\n",
    "*  immigration_table\n",
    "*  demgraphics_table\n",
    "*  time_table\n",
    "*  airport_table\n",
    "\n",
    "We then join the the dimension tables and create the below fact table.\n",
    "* us_city_immigration\n",
    "\n",
    "The reason for choosing star schema is that the source data was at grain needed for the project. Dim tables has Pk to identify each row and could be brought to fact table as Fk.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The source files are stored in a S3 bucket.\n",
    "Using spark on Amazon EMR, the source files are extracted, transformed and loaded to Parquet.\n",
    "Parquet files are written to s3.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.1.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "#df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read source to spark dataframe\n",
    "df_temp=spark.read.csv('../../data2/GlobalLandTemperaturesByCity.csv',header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------------------+-------+-------------+-----+\n",
      "|        dt| average_temperature|average_temperature_uncertainty|   city|      country|month|\n",
      "+----------+--------------------+-------------------------------+-------+-------------+-----+\n",
      "|2012-01-01|               7.996|                          0.204|Abilene|United States|    1|\n",
      "|2012-02-01|               8.434|                          0.252|Abilene|United States|    2|\n",
      "|2012-03-01|              15.628|            0.17300000000000001|Abilene|United States|    3|\n",
      "|2012-04-01|  21.069000000000003|            0.38799999999999996|Abilene|United States|    4|\n",
      "|2012-05-01|              24.698|            0.32299999999999995|Abilene|United States|    5|\n",
      "|2012-06-01|              28.217|                          0.126|Abilene|United States|    6|\n",
      "|2012-07-01|              29.581|            0.28800000000000003|Abilene|United States|    7|\n",
      "|2012-08-01|  29.104000000000006|                          0.322|Abilene|United States|    8|\n",
      "|2012-09-01|  24.333000000000002|                          0.313|Abilene|United States|    9|\n",
      "|2012-10-01|              16.702|                          0.264|Abilene|United States|   10|\n",
      "|2012-11-01|  13.892000000000001|            0.28600000000000003|Abilene|United States|   11|\n",
      "|2012-12-01|               7.951|            0.28600000000000003|Abilene|United States|   12|\n",
      "|2012-01-01|-0.34399999999999986|                           0.41|  Akron|United States|    1|\n",
      "|2012-02-01|  1.5269999999999997|                          0.319|  Akron|United States|    2|\n",
      "|2012-03-01|              10.109|                          0.442|  Akron|United States|    3|\n",
      "|2012-04-01|               9.195|            0.41200000000000003|  Akron|United States|    4|\n",
      "|2012-05-01|              18.921|                          0.322|  Akron|United States|    5|\n",
      "|2012-06-01|              21.108|                          0.298|  Akron|United States|    6|\n",
      "|2012-07-01|  24.965999999999998|                          0.401|  Akron|United States|    7|\n",
      "|2012-08-01|               21.94|            0.32799999999999996|  Akron|United States|    8|\n",
      "+----------+--------------------+-------------------------------+-------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cleansing and transforming temperature data\n",
    "df_temp = df_temp.drop('Latitude','Longitude').filter(\"Country == 'United States' AND dt > '2011-12-01' AND dt < '2013-01-01'\").withColumn('month',month(to_date(col('dt'))))\\\n",
    "        .select(col('dt'),col('AverageTemperature').alias('average_temperature'),col('AverageTemperatureUncertainty').alias('average_temperature_uncertainty')\\\n",
    "        ,col('City').alias('city'),col('Country').alias('country'),col('month'))\n",
    "df_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- average_temperature: string (nullable = true)\n",
      " |-- average_temperature_uncertainty: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                                 0\n",
       "average_temperature                0\n",
       "average_temperature_uncertainty    0\n",
       "city                               0\n",
       "country                            0\n",
       "month                              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check again for any nulls in the data after data transformation\n",
    "df_pd = df_temp.toPandas()\n",
    "df_pd.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read airport data to spark dataframe\n",
    "df_airport=spark.read.csv('./airport-codes_csv.csv',header='true')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Cleaning and transformingt he airport data\n",
    "df_airport = df_airport.withColumn('country',split(col(\"iso_region\"),\"-\").getItem(0)).withColumn(\"state\",split(col(\"iso_region\"),\"-\").getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+---------+-----------+----------+------------+---------+-------+-----+----------------+\n",
      "|ident|          type|                name|continent|iso_country|iso_region|municipality|iata_code|country|state|new_municipality|\n",
      "+-----+--------------+--------------------+---------+-----------+----------+------------+---------+-------+-----+----------------+\n",
      "| KABE|medium_airport|Lehigh Valley Int...|       NA|         US|     US-PA|   Allentown|      ABE|     US|   PA|       Allentown|\n",
      "| KOKK| small_airport|Kokomo Municipal ...|       NA|         US|     US-IN|      Kokomo|      OKK|     US|   IN|          Kokomo|\n",
      "| PHKO|medium_airport|Ellison Onizuka K...|       NA|         US|     US-HI| Kailua/Kona|      KOA|     US|   HI|          Kailua|\n",
      "| KEUF| small_airport|        Weedon Field|       NA|         US|     US-AL|     Eufaula|      EUF|     US|   AL|         Eufaula|\n",
      "| KLNS|medium_airport|   Lancaster Airport|       NA|         US|     US-PA|   Lancaster|      LNS|     US|   PA|       Lancaster|\n",
      "| KMEM| large_airport|Memphis Internati...|       NA|         US|     US-TN|     Memphis|      MEM|     US|   TN|         Memphis|\n",
      "| KOWD|medium_airport|Norwood Memorial ...|       NA|         US|     US-MA|     Norwood|      OWD|     US|   MA|         Norwood|\n",
      "|  LRO|        closed|          Sharpe AAF|       NA|         US|     US-CA|     Lathrop|      LRO|     US|   CA|         Lathrop|\n",
      "| PAHL|medium_airport|      Huslia Airport|       NA|         US|     US-AK|      Huslia|      HSL|     US|   AK|          Huslia|\n",
      "|  A63| small_airport|  Twin Hills Airport|       NA|         US|     US-AK|  Twin Hills|      TWA|     US|   AK|      Twin Hills|\n",
      "| K3TR| small_airport|Jerry Tyler Memor...|       NA|         US|     US-MI|       Niles|      NLE|     US|   MI|           Niles|\n",
      "| KCCR| small_airport|      Buchanan Field|       NA|         US|     US-CA|     Concord|      CCR|     US|   CA|         Concord|\n",
      "| KCKC| small_airport|Grand Marais Cook...|       NA|         US|     US-MN|Grand Marais|      GRM|     US|   MN|    Grand Marais|\n",
      "| KFVE| small_airport|Northern Aroostoo...|       NA|         US|     US-ME| Frenchville|      WFK|     US|   ME|     Frenchville|\n",
      "| KIDA|medium_airport|Idaho Falls Regio...|       NA|         US|     US-ID| Idaho Falls|      IDA|     US|   ID|     Idaho Falls|\n",
      "| KJLN| large_airport|Joplin Regional A...|       NA|         US|     US-MO|      Joplin|      JLN|     US|   MO|          Joplin|\n",
      "| KMAF|medium_airport|Midland Internati...|       NA|         US|     US-TX|     Midland|      MAF|     US|   TX|         Midland|\n",
      "| KO27| small_airport|     Oakdale Airport|       NA|         US|     US-CA|     Oakdale|      ODC|     US|   CA|         Oakdale|\n",
      "|  U41| small_airport|Dubois Municipal ...|       NA|         US|     US-ID|      Dubois|      DBS|     US|   ID|          Dubois|\n",
      "| K7G9| small_airport|Canton Municipal ...|       NA|         US|     US-SD|      Canton|      CTK|     US|   SD|          Canton|\n",
      "+-----+--------------+--------------------+---------+-----------+----------+------------+---------+-------+-----+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport=df_airport.drop('coordinates','local_code','gps_code','elevation_ft').dropna(how = \"any\", subset = [\"municipality\",\"iata_code\"]).filter(\"iso_country == 'US'\")\n",
    "df_airport=df_airport.withColumn(\"new_municipality\",split(col(\"municipality\"),\"/\").getItem(0)).distinct()\n",
    "df_airport.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident               0\n",
       "type                0\n",
       "name                0\n",
       "continent           0\n",
       "iso_country         0\n",
       "iso_region          0\n",
       "municipality        0\n",
       "iata_code           0\n",
       "country             0\n",
       "state               0\n",
       "new_municipality    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for nulls again after data transformation\n",
    "df_pd = df_airport.toPandas()\n",
    "df_pd.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read demographics data to spark dataframe\n",
    "df_us_cities=spark.read.option(\"sep\",\";\").csv('./us-cities-demographics.csv',header='true')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- total_population: string (nullable = true)\n",
      " |-- foreign_born: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cleanse and transform demographics data\n",
    "df_us_cities=df_us_cities.select(col('City').alias('city'),col('State').alias('state'),col('Total Population').alias('total_population'),col('Foreign-born').alias('foreign_born'),col('State Code').alias('state_code')).dropna().distinct()\n",
    "df_us_cities.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                0\n",
       "State               0\n",
       "total_population    0\n",
       "foreign_born        0\n",
       "state_code          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for nulls again after data transformation\n",
    "df_pd = df_us_cities.toPandas()\n",
    "df_pd.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- arrival_date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read immigration data to spark dataframe\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "df_spark.count()\n",
    "\n",
    "#Function to convert SAS date to date format\n",
    "get_date = udf(lambda x:datetime.datetime(1960,1,1) + datetime.timedelta(days=int(x)),TimestampType())\n",
    "\n",
    "#Cleanse and transform immigration data\n",
    "df_spark = df_spark.withColumn('arrival_date', get_date(df_spark.arrdate))\\\n",
    ".dropna(how = \"any\", subset = [\"i94mode\",\"i94addr\",\"i94bir\"])\\\n",
    ".select('cicid','i94yr','i94mon','i94cit','i94res','i94port','arrdate','i94mode','i94addr','i94bir','i94visa','biryear','visatype','arrival_date')\n",
    "#df_spark.show()\n",
    "df_spark.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+------+-------+-------+--------+-------------------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|i94bir|i94visa|biryear|visatype|       arrival_date|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+------+-------+-------+--------+-------------------+\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|  25.0|    3.0| 1991.0|      F1|2016-04-07 00:00:00|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|  55.0|    2.0| 1961.0|      B2|2016-04-01 00:00:00|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|  28.0|    2.0| 1988.0|      B2|2016-04-01 00:00:00|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|   4.0|    2.0| 2012.0|      B2|2016-04-01 00:00:00|\n",
      "| 18.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MI|  57.0|    1.0| 1959.0|      B1|2016-04-01 00:00:00|\n",
      "| 19.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|  63.0|    2.0| 1953.0|      B2|2016-04-01 00:00:00|\n",
      "| 20.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|  57.0|    2.0| 1959.0|      B2|2016-04-01 00:00:00|\n",
      "| 21.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|  46.0|    2.0| 1970.0|      B2|2016-04-01 00:00:00|\n",
      "| 22.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|  48.0|    1.0| 1968.0|      B1|2016-04-01 00:00:00|\n",
      "| 23.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NY|  52.0|    2.0| 1964.0|      B2|2016-04-01 00:00:00|\n",
      "| 24.0|2016.0|   4.0| 101.0| 101.0|    TOR|20545.0|    1.0|     MO|  33.0|    2.0| 1983.0|      B2|2016-04-01 00:00:00|\n",
      "| 27.0|2016.0|   4.0| 101.0| 101.0|    BOS|20545.0|    1.0|     MA|  58.0|    1.0| 1958.0|      B1|2016-04-01 00:00:00|\n",
      "| 28.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     MA|  56.0|    1.0| 1960.0|      B1|2016-04-01 00:00:00|\n",
      "| 29.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     MA|  62.0|    2.0| 1954.0|      B2|2016-04-01 00:00:00|\n",
      "| 30.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     NJ|  49.0|    2.0| 1967.0|      B2|2016-04-01 00:00:00|\n",
      "| 31.0|2016.0|   4.0| 101.0| 101.0|    ATL|20545.0|    1.0|     NY|  43.0|    2.0| 1973.0|      B2|2016-04-01 00:00:00|\n",
      "| 33.0|2016.0|   4.0| 101.0| 101.0|    HOU|20545.0|    1.0|     TX|  53.0|    2.0| 1963.0|      B2|2016-04-01 00:00:00|\n",
      "| 34.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     CT|  48.0|    2.0| 1968.0|      B2|2016-04-01 00:00:00|\n",
      "| 35.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     CT|  74.0|    2.0| 1942.0|      B2|2016-04-01 00:00:00|\n",
      "| 36.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     NJ|  37.0|    2.0| 1979.0|      B2|2016-04-01 00:00:00|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+------+-------+-------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cicid           0\n",
       "i94yr           0\n",
       "i94mon          0\n",
       "i94cit          0\n",
       "i94res          0\n",
       "i94port         0\n",
       "arrdate         0\n",
       "i94mode         0\n",
       "i94addr         0\n",
       "i94bir          0\n",
       "i94visa         0\n",
       "biryear         0\n",
       "visatype        0\n",
       "arrival_date    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check again for nulls \n",
    "df_pd = df_spark.toPandas()\n",
    "df_pd.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+---+----+-----+----+---------+\n",
      "|               date|hour|day|week|month|year|  weekday|\n",
      "+-------------------+----+---+----+-----+----+---------+\n",
      "|2016-04-02 00:00:00|   0|  2|  13|    4|2016| Saturday|\n",
      "|2016-04-15 00:00:00|   0| 15|  15|    4|2016|   Friday|\n",
      "|2016-04-18 00:00:00|   0| 18|  16|    4|2016|   Monday|\n",
      "|2016-04-16 00:00:00|   0| 16|  15|    4|2016| Saturday|\n",
      "|2016-04-11 00:00:00|   0| 11|  15|    4|2016|   Monday|\n",
      "|2016-04-14 00:00:00|   0| 14|  15|    4|2016| Thursday|\n",
      "|2016-04-17 00:00:00|   0| 17|  15|    4|2016|   Sunday|\n",
      "|2016-04-23 00:00:00|   0| 23|  16|    4|2016| Saturday|\n",
      "|2016-04-26 00:00:00|   0| 26|  17|    4|2016|  Tuesday|\n",
      "|2016-04-10 00:00:00|   0| 10|  14|    4|2016|   Sunday|\n",
      "|2016-04-24 00:00:00|   0| 24|  16|    4|2016|   Sunday|\n",
      "|2016-04-01 00:00:00|   0|  1|  13|    4|2016|   Friday|\n",
      "|2016-04-21 00:00:00|   0| 21|  16|    4|2016| Thursday|\n",
      "|2016-04-05 00:00:00|   0|  5|  14|    4|2016|  Tuesday|\n",
      "|2016-04-08 00:00:00|   0|  8|  14|    4|2016|   Friday|\n",
      "|2016-04-13 00:00:00|   0| 13|  15|    4|2016|Wednesday|\n",
      "|2016-04-25 00:00:00|   0| 25|  17|    4|2016|   Monday|\n",
      "|2016-04-07 00:00:00|   0|  7|  14|    4|2016| Thursday|\n",
      "|2016-04-09 00:00:00|   0|  9|  14|    4|2016| Saturday|\n",
      "|2016-04-19 00:00:00|   0| 19|  16|    4|2016|  Tuesday|\n",
      "+-------------------+----+---+----+-----+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a dataframe for time \n",
    "df_time = df_spark.select(col('arrival_date').alias('date'),\\\n",
    "              hour(col('arrival_date')).alias('hour'),\\\n",
    "              dayofmonth(col('arrival_date')).alias('day'),\\\n",
    "              weekofyear(col('arrival_date')).alias('week'),\\\n",
    "              month(col('arrival_date')).alias('month'),\\\n",
    "              year(col('arrival_date')).alias('year'),\\\n",
    "              date_format(col('arrival_date'),\"EEEE\").alias('weekday'),\\\n",
    "             ).dropDuplicates()\n",
    "df_time.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Write immigration data to parquet file\n",
    "df_spark.write.partitionBy(\"i94yr\",\"i94mon\",\"i94port\").parquet(\"sas_data\",'overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Write temperature data to parquet file\n",
    "df_temp.write.partitionBy(\"city\").parquet(\"temperature_data\",'overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Write airport data to parquet file\n",
    "df_airport.write.partitionBy(\"iata_code\").parquet(\"airport_data\",'overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Write city data to parquet file\n",
    "df_us_cities.write.partitionBy(\"city\").parquet(\"cities_data\",'overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Write time data to parquet file\n",
    "df_time.write.partitionBy(\"year\",\"month\").parquet(\"time_data\",'overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read data from parquet file\n",
    "df_spark=spark.read.parquet(\"sas_data\")\n",
    "df_temp=spark.read.parquet(\"temperature_data\")\n",
    "df_airport=spark.read.parquet(\"airport_data\")\n",
    "df_us_cities=spark.read.parquet(\"cities_data\")\n",
    "df_time=spark.read.parquet(\"time_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Quality checks to see if the dimension tables are empty\n",
    "if df_temp.count() == 0:\n",
    "    print(\"Temperature data is empty\")\n",
    "    raise ValueError\n",
    "if df_airport.count() == 0:\n",
    "    print(\"Airport data is empty\")\n",
    "    raise ValueError\n",
    "if df_us_cities.count() == 0:\n",
    "    print(\"Demography data is empty\")\n",
    "    raise ValueError\n",
    "\n",
    "if df_time.count() == 0:\n",
    "    print(\"Time data is empty\")\n",
    "    raise ValueError\n",
    "\n",
    "\n",
    "if df_spark.count() == 0:\n",
    "    print(\"Immigration data is empty\")\n",
    "    raise ValueError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------+-----+-----+-----+-------+-------------------+------------+\n",
      "|fact_id|               date|    cicid|ident|month| city|  state|average_temperature|foreign_born|\n",
      "+-------+-------------------+---------+-----+-----+-----+-------+-------------------+------------+\n",
      "|      0|2016-04-15 00:00:00|2688864.0| KMIA|    4|Miami|Florida|             23.115|      260789|\n",
      "|      1|2016-04-15 00:00:00|2688885.0| KMIA|    4|Miami|Florida|             23.115|      260789|\n",
      "|      2|2016-04-15 00:00:00|2688912.0| KMIA|    4|Miami|Florida|             23.115|      260789|\n",
      "|      3|2016-04-15 00:00:00|2688920.0| KMIA|    4|Miami|Florida|             23.115|      260789|\n",
      "|      4|2016-04-15 00:00:00|2688921.0| KMIA|    4|Miami|Florida|             23.115|      260789|\n",
      "|      5|2016-04-15 00:00:00|2688922.0| KMIA|    4|Miami|Florida|             23.115|      260789|\n",
      "|      6|2016-04-15 00:00:00|2688964.0| KMIA|    4|Miami|Florida|             23.115|      260789|\n",
      "|      7|2016-04-15 00:00:00|2688965.0| KMIA|    4|Miami|Florida|             23.115|      260789|\n",
      "|      8|2016-04-15 00:00:00|2688966.0| KMIA|    4|Miami|Florida|             23.115|      260789|\n",
      "|      9|2016-04-15 00:00:00|2688967.0| KMIA|    4|Miami|Florida|             23.115|      260789|\n",
      "|     10|2016-04-15 00:00:00|2688968.0| KMIA|    4|Miami|Florida|             23.115|      260789|\n",
      "|     11|2016-04-15 00:00:00|2688969.0| KMIA|    4|Miami|Florida|             23.115|      260789|\n",
      "|     12|2016-04-15 00:00:00|2688970.0| KMIA|    4|Miami|Florida|             23.115|      260789|\n",
      "|     13|2016-04-15 00:00:00|2688971.0| KMIA|    4|Miami|Florida|             23.115|      260789|\n",
      "|     14|2016-04-15 00:00:00|2688972.0| KMIA|    4|Miami|Florida|             23.115|      260789|\n",
      "|     15|2016-04-15 00:00:00|2688973.0| KMIA|    4|Miami|Florida|             23.115|      260789|\n",
      "|     16|2016-04-15 00:00:00|2688974.0| KMIA|    4|Miami|Florida|             23.115|      260789|\n",
      "|     17|2016-04-15 00:00:00|2688975.0| KMIA|    4|Miami|Florida|             23.115|      260789|\n",
      "|     18|2016-04-15 00:00:00|2688976.0| KMIA|    4|Miami|Florida|             23.115|      260789|\n",
      "|     19|2016-04-15 00:00:00|2688977.0| KMIA|    4|Miami|Florida|             23.115|      260789|\n",
      "+-------+-------------------+---------+-----+-----+-----+-------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create fact tables by joining the dimension tables\n",
    "df_fact=df_spark.join(df_airport,[df_spark.i94port == df_airport.iata_code],how='inner').select(df_spark.arrival_date,df_spark.cicid,df_spark.i94mon,df_airport.new_municipality,df_airport.state,df_airport.ident)\\\n",
    "            .join(df_temp,[df_airport.new_municipality == df_temp.city,df_spark.i94mon == df_temp.month.cast(\"double\")], how='inner')\\\n",
    "            .join(df_us_cities,[df_temp.city == df_us_cities.city,df_airport.state == df_us_cities.state_code],how ='inner')\\\n",
    "            .join(df_time,[df_spark.arrival_date == df_time.date],how='inner')\\\n",
    "            .withColumn(\"fact_id\", monotonically_increasing_id())\\\n",
    "            .select(col('fact_id'),df_time.date,df_spark.cicid,df_airport.ident,df_temp.month,df_temp.city,df_us_cities.state,df_temp.average_temperature,df_us_cities.foreign_born)\n",
    "    \n",
    "df_fact.show()\n",
    "#1218050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_fact.write.partitionBy(\"month\",\"city\").parquet(\"immigration_weather\",'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_fact=spark.read.parquet(\"immigration_weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fact_id: long (nullable = false)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- ident: string (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- average_temperature: string (nullable = true)\n",
      " |-- foreign_born: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#type(df_fact)\n",
    "df_fact.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "As part of the data quality check the following  are done:\n",
    "\n",
    " * Check if the final fact table is empty.\n",
    " * Check if any columns are null in the fact table.\n",
    " * Check if dimension table after its read from parquet file is empty.\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check if fact table is empty\n",
    "if  df_fact.count() == 0:\n",
    "    print(\"No data in the fact table\")\n",
    "    raise ValueError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check for null values in the fact table\n",
    "if df_fact.filter(\"date is null or cicid is null or ident is null or month is null or city is null or state is null\").count() != 0:\n",
    "    print(\"Pk column cannot be null\")\n",
    "    raise ValueError\n",
    "if df_fact.filter(\"average_temperature is null or foreign_born is null or city is null\").count() != 0:\n",
    "    print(\"Column cannot be null\")\n",
    "    raise ValueError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Refer to the file Data_Dictionary.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    " \n",
    " \n",
    " * Tools and technologies for the project\n",
    " \n",
    " For the project the technologies used are Spark, S3 and EMR.The reason for using them\n",
    "\n",
    " EMR - EMR comes preinstalled with bigdata eco system that we specify.With EMR we can choose the node types ie Compute Optimized Nodes or Storage OPtimized Nodes.We can scale up or scale down the number of nodes for the job.\n",
    "\n",
    " Spark - Spark works well with large dataset. It works faster with its in-memory processing and distributed architecure making use of parallel processing.\n",
    " \n",
    "  S3  - It provides easy, low cost ,scalable storage option.The final fact and dimension table are stored as parquet file for easier access and S3 provides a good home to store parquet file.\n",
    "  \n",
    "* Propose how often the data should be updated and why\n",
    "\n",
    "The data could be updated monthly once so that we can retrieve the last month's temperature data set for each city and then join it with the i94 port to know if last months weather had impacted the visitor flow to that city.\n",
    "  \n",
    "* Approaches for the different scenario \n",
    " * The data was increased by 100x.\n",
    " \n",
    " When the data scales up we can improve the performance  by increasing the number of nodes for processing.\n",
    " In the case where S3 and EMR are used we can make sure that S3 and EC2 are in the same region.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " \n",
    " We can use a scheduler like Airflow to run the job daily so that dashboard is refershed with the new data everyday.\n",
    " * The database needed to be accessed by 100+ people.\n",
    "   \n",
    "  Here the fact tables and fact tables are stored as parquet files in S3. So we need to optimize S3 reads.S3 supports 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket. There are no limits to the number of prefixes in a bucket. We canincrease our read or write performance by parallelizing reads. We can add more prefixes in an Amazon S3 bucket to parallelize reads,we could scale ourread performance .\n",
    " We can even use Amazon CloudFront, AmazonElastiCache, or AWS Elemental MediaStore to store common set of objects that gets repeated GET requests.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 6 Analytics performed on the fact table\n",
    "  In the jupyter notebook I have considered only April immigration data. Hence all the below analytics is for April "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_fact.createOrReplaceTempView(\"fact_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------+-------------------+------------+------+\n",
      "|month|          city|         state|average_temperature|foreign_born|   cnt|\n",
      "+-----+--------------+--------------+-------------------+------------+------+\n",
      "|    4|         Miami|       Florida|             23.115|      260789|330265|\n",
      "|    4|       Orlando|       Florida| 22.976999999999997|       50558|154848|\n",
      "|    4|   Los Angeles|    California|             14.751|     1485425|148635|\n",
      "|    4|   New Orleans|     Louisiana| 22.101999999999997|       21679|133374|\n",
      "|    4|       Houston|         Texas| 22.468000000000004|      696210| 95774|\n",
      "|    4|       Atlanta|       Georgia|             16.287|       32016| 89237|\n",
      "|    4|        Dallas|         Texas|             20.552|      326825| 68277|\n",
      "|    4|        Boston| Massachusetts|              8.753|      190123| 55795|\n",
      "|    4|       Seattle|    Washington|              7.063|      119840| 46445|\n",
      "|    4|       Detroit|      Michigan|              8.351|       39861| 35904|\n",
      "|    4|        Denver|      Colorado|             10.807|      113222| 17491|\n",
      "|    4|     Charlotte|North Carolina|             17.456|      128897| 15432|\n",
      "|    4|     Santa Ana|    California| 15.094000000000001|      152999|  6424|\n",
      "|    4|Salt Lake City|          Utah|             11.296|       32166|  5013|\n",
      "|    4|       Oakland|    California|              14.15|      113896|  3468|\n",
      "|    4|        Austin|         Texas| 22.441000000000006|      181686|  2977|\n",
      "|    4|    Sacramento|    California|              14.15|      112579|  2103|\n",
      "|    4|       Raleigh|North Carolina|             14.876|       65125|  2084|\n",
      "|    4|       Buffalo|      New York|               6.69|       24630|   909|\n",
      "|    4|       Ontario|    California| 16.570999999999998|       48557|   773|\n",
      "+-----+--------------+--------------+-------------------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# city with maximum visitors \n",
    "spark.sql(\"SELECT month,city,state, average_temperature,foreign_born,count(*) as cnt FROM fact_table group by 1,2,3,4,5 order by cnt desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------+-------------------+------------+---+\n",
      "|month|          city|         state|average_temperature|foreign_born|cnt|\n",
      "+-----+--------------+--------------+-------------------+------------+---+\n",
      "|    4|  Newport News|      Virginia|             14.108|       12589|  1|\n",
      "|    4|    Montgomery|       Alabama|             18.724|        9337|  1|\n",
      "|    4|Corpus Christi|         Texas| 24.026999999999997|       30834|  2|\n",
      "|    4|   Albuquerque|    New Mexico|             13.133|       58200|  3|\n",
      "|    4|        Mobile|       Alabama|             19.913|        7234|  3|\n",
      "|    4|       Chicago|      Illinois|  8.383000000000001|      573463|  3|\n",
      "|    4|    Huntsville|       Alabama|             16.933|       12691|  4|\n",
      "|    4|       Wichita|        Kansas| 16.480999999999998|       40270|  5|\n",
      "|    4|       Lansing|      Michigan|              8.625|        8371|  6|\n",
      "|    4|    Manchester| New Hampshire|              8.753|       14506|  8|\n",
      "|    4|     Rochester|     Minnesota|               6.69|       17763|  9|\n",
      "|    4|         Omaha|      Nebraska|             13.966|       48263| 10|\n",
      "|    4|    Charleston|South Carolina|             19.461|        5767| 13|\n",
      "|    4|   Chattanooga|     Tennessee|             16.933|       10599| 13|\n",
      "|    4|      Columbia|South Carolina|             17.456|        6074| 14|\n",
      "|    4|      Hartford|   Connecticut|             10.372|       28467| 14|\n",
      "|    4|      Savannah|       Georgia|             20.353|       10355| 23|\n",
      "|    4|    Fort Wayne|       Indiana|             10.368|       19146| 30|\n",
      "|    4|       Memphis|     Tennessee| 17.970000000000006|       43318| 36|\n",
      "|    4|       Spokane|    Washington|  8.495000000000001|       13253| 64|\n",
      "+-----+--------------+--------------+-------------------+------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#City with least visitors\n",
    "spark.sql(\"SELECT month,city,state, average_temperature,foreign_born,count(*) as cnt FROM fact_table group by 1,2,3,4,5 order by cnt asc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+----------+-------------------+------------+------+----+\n",
      "|month|       city|     state|average_temperature|foreign_born|   cnt|rank|\n",
      "+-----+-----------+----------+-------------------+------------+------+----+\n",
      "|    4|      Miami|   Florida|             23.115|      260789|330265|   1|\n",
      "|    4|    Orlando|   Florida| 22.976999999999997|       50558|154848|   2|\n",
      "|    4|Los Angeles|California|             14.751|     1485425|148635|   3|\n",
      "+-----+-----------+----------+-------------------+------------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#count of immigrants  city in each month(The data has only April data and hence shows up only april data)\n",
    "spark.sql(\"select * from(select *,  rank() OVER (PARTITION BY month ORDER BY cnt DESC) rank from (SELECT month,city,state, average_temperature,foreign_born,count(*) as cnt FROM fact_table group by 1,2,3,4,5) as tbl )as tbl1 where rank <=3 order by month,rank\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark.createOrReplaceTempView(\"immigration_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----------+-------------+-------------------+------------+------+----+\n",
      "|month|i94mode|       city|        state|average_temperature|foreign_born|   cnt|rank|\n",
      "+-----+-------+-----------+-------------+-------------------+------------+------+----+\n",
      "|    4|    1.0|      Miami|      Florida|             23.115|      260789|328422|   1|\n",
      "|    4|    1.0|    Orlando|      Florida| 22.976999999999997|       50558|153862|   2|\n",
      "|    4|    1.0|Los Angeles|   California|             14.751|     1485425|148195|   3|\n",
      "|    4|    1.0|New Orleans|    Louisiana| 22.101999999999997|       21679|133054|   4|\n",
      "|    4|    1.0|    Houston|        Texas| 22.468000000000004|      696210| 95194|   5|\n",
      "|    4|    2.0|      Miami|      Florida|             23.115|      260789|  1059|   1|\n",
      "|    4|    2.0|    Houston|        Texas| 22.468000000000004|      696210|   336|   2|\n",
      "|    4|    2.0|    Seattle|   Washington|              7.063|      119840|   170|   3|\n",
      "|    4|    2.0|    Orlando|      Florida| 22.976999999999997|       50558|   110|   4|\n",
      "|    4|    2.0|Los Angeles|   California|             14.751|     1485425|    68|   5|\n",
      "|    4|    3.0|    Detroit|     Michigan|              8.351|       39861|  2070|   1|\n",
      "|    4|    3.0|    Orlando|      Florida| 22.976999999999997|       50558|   870|   2|\n",
      "|    4|    3.0|    Buffalo|     New York|               6.69|       24630|   706|   3|\n",
      "|    4|    3.0|      Miami|      Florida|             23.115|      260789|   613|   4|\n",
      "|    4|    3.0|Los Angeles|   California|             14.751|     1485425|   329|   5|\n",
      "|    4|    9.0|      Miami|      Florida|             23.115|      260789|   171|   1|\n",
      "|    4|    9.0|    Buffalo|     New York|               6.69|       24630|   129|   2|\n",
      "|    4|    9.0|    Detroit|     Michigan|              8.351|       39861|   105|   3|\n",
      "|    4|    9.0|Los Angeles|   California|             14.751|     1485425|    43|   4|\n",
      "|    4|    9.0|     Boston|Massachusetts|              8.753|      190123|    43|   4|\n",
      "+-----+-------+-----------+-------------+-------------------+------------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Rank the cities according to their visitor count per the i94mode and get the first 5 ranks\n",
    "# i94mode = 1 -> Business\n",
    "# i94mode = 2 -> Pleasure\n",
    "# i94mode = 3 -> Student\n",
    "stmnt = \"select * from\\\n",
    "(Select * , rank() over (PARTITION BY month,i94mode ORDER BY cnt DESC) rank from \\\n",
    "(SELECT month,i94mode,city,state, average_temperature,foreign_born,count(*) as cnt \\\n",
    "FROM fact_table join immigration_table on fact_table.cicid = \\\n",
    "immigration_table.cicid group by 1,2,3,4,5,6) as temp) \\\n",
    "as temp1 where rank<=5 order by month,i94mode,rank\"\n",
    "spark.sql(stmnt).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
